\section{Resultados}

Teniendo en cuenta los criterios de inclusión planteados anteriormente, en la tabla \ref{metodologias} se encuentra una síntesis de los resultados obtenidos de cada una de las  metodologías que se consideraron relevantes para la revisión sistemática. Cabe resaltar, que ninguna de las metodologías analizadas se enfoca directamente en el cáncer de mama, sin embargo hacen un gran énfasis en el entendimiento del dominio y su importancia en la definición de metas claras y alcanzables para dar valor a los datos. 

\citep{Schroer2021} realizan una descripción general del enfoque de la investigación, las metodologías actuales, las mejores prácticas y las posibles brechas en la ejecución de las fases de la metodología para minería de datos \textit{CRISP-DM} \footnote{Cross-Industry Standard Process for Data Mining }. Esta metodología es una innovación cruzada entre industrias de diferentes sectores. Fue desarrollada por SPSS y Teradata en 1996 y describe un enfoque  que es comúnmente utilizado por expertos en minería de datos. Esta metodología presenta un proceso iterativo estructurado, bien definido y documentado que consta de seis fases iterativas: comprensión del negocio, comprensión de datos, preparación de datos, modelado, evaluación y despliegue. Adicionalmente, esta metodología se divide en tareas genéricas con objetivos específicos y resultados concretos. Estos hitos permiten la evaluación intermedia de los resultados, una eventual re-planificación y una colaboración más fácil. Las tareas genéricas pretenden abarcar el mayor número posible de situaciones en la minería de datos, y se dividen a su vez en tareas o actividades específicas \citep{Mladenic2012}. En conclusión, dado que esta esta metodología abarca desde la comprensión empresarial hasta la implementación, y ademas se compone de un proceso fácil y estructurado, confiable, de uso común e independiente de la industria, es la metodología mas popular en la práctica y en la investigación.

\cite{Safhi2019} estudiaron y evaluaron la metodología KDD \footnote{Knowledge Discovery Databases}, la cual tiene como propósito principal  extraer información previamente desconocida y patrones ocultos comprensibles en los datos. Esta metodología consta de cinco etapas: selección de datos,  pre-procesamiento de datos, transformación de datos , minería de datos y evaluación/interpretación. En esta metodología los datos se obtienen de múltiples fuentes, y cada una de las etapas del proceso puede ser aplicada a diferentes organizaciones. En conclusión, esta metodología se adapta al crecimiento exponencial de los datos y ayuda a que las diferentes organizaciónes generen valor a dichos datos basándose en un entendimiento previo del dominio.

\citep{Shafique2014} realizaron un estudio comparativo y una descripción puntual de los aspectos mas importantes  de la metodología denominada SEMMA\footnote{Sample, Explore, Modify, Model, and Access}. Esta metodología fue creada por la organización SAS Enterprise Miner y permite comprender, organizar, desarrollar y mantener proyectos de minería de datos a través de un ciclo de cinco etapas: muestreo, exploración, modificación, modelado y evaluación de datos.  En conclusión, esta metodología ofrece un ciclo de vida para la solución de los problemas típicos del ámbito empresarial al tratar de cumplir objetivos que necesiten modelos predictivos y descriptivos para el análisis de grandes volúmenes de datos.

\cite{Mladenic2012} proponen una metodología denominada RAMSYS \footnote{Remote Collaborative Data Mining System}, para llevar a cabo proyectos de minería de datos a distancia de manera colaborativa . Esta metodología se compone de seis fases: gestión del negocio,  libertad para resolver problemas, empezar en cualquier momento, parar en cualquier momento, intercambio de conocimientos en línea y seguridad. Esta metodología profundiza las fases de CRISP-DM, y permite que el esfuerzo de minería de datos se invierta en diferentes ubicaciones que se comunican a través de una herramienta basada en la web. En conclusión, el objetivo de la metodología es permitir el trabajo colaborativo de científicos de datos ubicados de forma remota, de modo que se facilite el intercambio de información a distancia, así como la libertad de experimentar con cualquier técnica de resolución de problemas \cite{Martinez2021}.

\citep{Elprin2022} fundadores de Domino Data Lab en Silicon Valley, crearon la metodología Domino DS, la cual abarca el ciclo de vida de un proyecto de ciencia de datos. Esta metodología se compone de las siguientes fases: ideación, adquisición y exploración de datos, investigación y desarrollo, validación, entrega y monitoreo. Esta metodología se fundamenta en CRISP-DM, la ágilidad y las necesidades del cliente para guiar a un equipo de análisis de datos hacia un mejor desempeño. En conclusión, esta metodología integra eficazmente el proceso de la ciencia de datos, la ingeniería de software y los enfoques ágiles para generar valor a los datos a través de entregas iterativas enfocadas en primera instancia en el problema comercial y después en la implementación  \citep{Martinez2021}.

\citep{Larson2016} realizaron un estudio, en donde analizaron cómo los principios y prácticas ágiles han evolucionado con la inteligencia empresarial. En este estudio los autores abordaron la metodología Agile BI Delivery Framework. Esta metodología sugiere que los desafíos a los que se enfrentan los proyectos de BI hacen que el enfoque Agile sea una respuesta atractiva debido a las semejanzas que existen entre ambas, por lo tanto sugieren que dichas metodologías pueden evolucionar en paralelo. Por el lado de la metodología BI \footnote{Business Intelligence} tenemos cinco etapas: descubrimiento, diseño, desarrollo, despliegue y entrega de valor. Dado lo anterior, los autores proponen las siguientes etapas para Agile BI Delivery Framework: alcance, adquisición de datos, análisis, desarrollo de modelos, validación e implementación. En esta metodología el alcance se centra en un marco de entrega ágil que aborda la influencia de la ciencia de datos en BI. En conclusión, el propósito de la metodología es aplicar la agilidad a los problemas comunes que se encuentran en los proyectos de BI al promover la interacción y la colaboración entre las partes interesadas, debido a que esto garantiza requisitos más claros, una comprensión de los datos, una responsabilidad conjunta y la obtención de resultados de mayor calidad. Por lo tanto, se dedica menos tiempo a intentar determinar los requisitos de información y se dedica más tiempo a descubrir conocimiento oculto en los datos.

\citep{Maass2021} plantean que el ML requiere una comprensión profunda de los dominios a los que se pueden aplicar modelos y algoritmos de aprendizaje profundo debido a que los datos determinan la funcionalidad de un sistema de información. Por lo tanto, consideran que se requiere una evaluación para determinar si los datos de entrenamiento son representativos del dominio. Dado lo anterior, afirman que el poder real de la ciencia de datos se hace evidente al aprovechar el ML en big data con el propósito de tomar decisiones que tenga un soporte solido en los datos para generar estrategias claves en la transformación digital. De lo contrario, si no se tiene una claridad absoluta del dominio podrían surgir problemas que podrían contribuir a sesgos y errores en los modelos de aprendizaje automático. Dado lo anterior, proponen una metodología de desarrollo de siete fases basada en la unión del modelado conceptual con el aprendizaje automático: Entendimiento del problema, Recopilación de datos, Ingeniería de datos, Entrenamiento del modelo, Optimización del modelo, Integración y evaluación del modelo y la Toma de decisiones analíticas. Los autores consideran que, los  modelos conceptuales son un \textit{lente} a través de la cual los seres humanos obtienen una representación mental intuitiva, fácil de entender, significativa, directa y natural de un dominio. Por el contrario, el aprendizaje automático utiliza los datos como un \textit{lente} a través de la cual obtiene representaciones internas sobre las regularidades de los datos tomados de un dominio. En consecuencia, la unión del modelado conceptual con el ML contribuye entre sí a: mejorar la calidad de los modelos de aprendizaje automático mediante el uso de modelos conceptuales durante la ingeniería de datos, el entrenamiento de modelos y las pruebas de modelos y mejorar la interpretabilidad de los algoritmos de aprendizaje automático mediante el uso de modelos conceptuales. Los modelos prácticos de ML solo son útiles dentro de un dominio determinado, como juegos, decisiones comerciales, atención médica, política o educación. Cuando se integran en los sistemas de información, los modelos de ML deben seguir las leyes, las regulaciones, los valores sociales, la moral y la ética del dominio, y obedecer los requisitos derivados de los objetivos comerciales. Esto destaca la necesidad de un modelo conceptual para ayudar a transformar ideas comerciales en representaciones estructuradas y, a veces, incluso formales, que pueden utilizarse como pautas precisas para el desarrollo de software. Por lo tanto, ayudan a estructurar los procesos de pensamiento de los expertos en el dominio y los ingenieros de software.  

\citep{Grady2017} presentan un enfoque de una metodología de proceso moderno para el análisis de Big data (BDA\footnote{Big data analytic}) que se alinea con los nuevos cambios tecnológicos e implementa la agilidad en el ciclo de vida del análisis avanzado y el desarrollo de sistemas de ML, para minimizar el tiempo necesario para alcanzar un resultado deseado. La metodología propuesta, basada en el modelo de proceso de BDA, se llama Data Science Edge (DSE), la cual los autores consideran que junto con las metodologías ágiles sirve como un modelo de proceso para el descubrimiento de conocimientos en ciencia de datos. El ciclo de vida de DSE se compone de cinco fases: Planificar, Recopilar, Seleccionar, Analizar y Actuar. Para que DSE se alineara con una metodología ágil se proporciono una guía en cada fase sobre qué actividades serían fundamentales para comenzar y generar de forma eficiente un producto mínimo viable aunque existiera la necesidad de un refinamiento para mejorar los análisis posteriores. Por consiguiente, el propósito de la metodología propuesta fue adoptar una filosofía ágil para el desarrollo del análisis, en donde los resultados obtenidos se compartan con más frecuencia para formar un circuito de retroalimentación de la opinión de las partes interesadas y utilizar esas necesidades para validar el estado actual e influir en su evolución hacia un estado final que cumpla con los requerimientos y objetivos de un dominio especifico.

\citep{Sfaxi2020} proponen una metodología denominada \textit{DECIDE\footnote{Decisional Big Data Methodology}} la cual se fundamenta en un diseño ágil basado en eventos y datos para proyectos decisionales de Big Data. El propósito de esta metodología es ayudar a las organizaciones a determinar los objetivos comerciales y analíticos deseados según la toma de decisiones con base al análisis de datos, en donde se debe: definir una estrategia de datos clara, encontrar a las personas adecuadas para llevar a cabo un cambio cultural impulsado por los datos y seguir la ética de la información. Esta metodología se basa en la  metodología DSRM\footnote{Design Science Research Methodology} que permite realizar investigaciones en ciencias del diseño en sistemas de información y ayuda a definir un modelo de proceso a seguir para diseñar una solución de sistemas de información, dependiendo del enfoque realizado para encontrar dicha solución. La metodología DSRM define principalmente cuatro enfoques posibles: centrado en el problema, centrado en objetivos, centrado en el diseño y desarrollo, e impulsado por cliente y contexto. En el caso de la metodología DECIDE se utiliza un enfoque centrado en el problema, que consta de las siguientes fases: identificación y motivación del problema, definición de los objetivos de la solución, diseño y desarrollo, demostración, evaluación y comunicación. Cabe resaltar que el valor de esta metodología esta basado en cinco fundamentos claves en la fase de diseño y desarrollo: Agilidad, Enfoque Bottom-up, Datos y eventos, Multi-arquitecturas y Multi-tecnologías. En conclusión, esta metodología está diseñada para respetar los conceptos y las mejores prácticas para la toma de decisiones soportada en Big data y para ser aplicada a grandes proyectos, con un tamaño de equipo significativo. 

\begin{table*}
	\begin{threeparttable}
		\caption{Características de las metodologías en ciencia de datos revisadas.}
		\label{metodologias}
		\begin{tabular}{ p{2cm} p{4cm} p{5cm} p{6cm}  } \toprule
			Autores 
			&Metodología           
			&Fases      
			&Resultados
			\\ \hline	
			%-----------------------------------------------------------------------------------
			\citep{Schroer2021}
			&CRISP-DM
			& \begin{enumerate}
				\item Comprensión del negocio
				\item Comprensión de datos
				\item Preparación de datos
				\item Modelado
				\item Evaluación
				\item Despliegue 
			\end{enumerate}
			& \begin{enumerate}
				\item Es una de las metodologías en ciencia de datos más usada para proyectos de análisis, minería de datos y ciencia de datos.
				\item Esta metodología no explica cómo deben organizarse equipos de trabajo para llevar a cabo procesos de gestión que se alineen con el software.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Mladenic2012}
			&RAMSYS
			& \begin{enumerate}
			\item Gestión del negocio
			\item Libertad para resolver problemas
			\item Empezar en cualquier momento
			\item Parar en cualquier momento
			\item Intercambio de conocimientos en línea 
			\item Seguridad
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología se basa en la metodología CRISP-DM y permite la colaboración para proyectos de minería de datos desde diferentes ubicaciones por medio de una herramienta basada en la web.
				\item Esta metodología permite el trabajo colaborativo de científicos de datos ubicados de forma remota de una manera disciplinada en lo que respecta al flujo de información.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Microsoft2022}
			&Microsoft TDSP
			& \begin{enumerate}
				\item Comprensión empresarial
				\item Adquisición y comprensión de datos 
				\item Modelado
				\item Implementación
				\item Aceptación del cliente
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología ayuda a mejorar la colaboración y el aprendizaje en equipo.
				\item Esta metodología se preocupa por definir objetivos SMART (Específicos, medibles, alcanzables, relevante, con límite de tiempo).
				\item Esta metodología aborda la debilidad de la falta de definición del equipo de CRISP-DM mediante la creación de roles y sus responsabilidades durante cada fase del ciclo de vida del proyecto.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Martinez2021}
			&Domino DS Lifecycle
			& \begin{enumerate}
				\item Ideación
				\item Adquisición y exploración de datos
				\item Investigación y desarrollo
				\item Validación
				\item Entrega
				\item Monitoreo
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología se basa en la metodología CRISP-DM y en el manifiesto ágil.
				\item Esta metodología propone establecer un seguimiento para las entregas de la información y de los KPI comerciales.
				\item Esta metodología hace uso de grupos de control en modelos de producción para realizar un seguimiento del desempeño del modelo y la creación de valor para la empresa.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Larson2016}
			&Agile Delivery Framework
			&  \begin{enumerate}
				\item Alcance
				\item Adquisición de datos
				\item Análisis
				\item Desarrollo de modelos
				\item Validación
				\item Implementación 
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología está diseñada para fomentar la colaboración exitosa entre las empresas y las partes interesadas del proyecto.
				\item Esta metodología separa completamente el mundo de la inteligencia empresarial y el del análisis de datos. De hecho, propone dos metodologías que evolucionan en paralelo y mediante métodos ágiles promete una colaboración eficaz entre estas dos partes.
			\end{enumerate}
			
		\end{tabular}
	\end{threeparttable}
\end{table*}

\begin{table*}
	\begin{threeparttable}	
		\begin{tabular}{ p{2cm} p{4cm} p{5cm} p{6cm}  } \toprule
			%--------------------------------------------------------------------------------
			\citep{Maass2021}
			&Conceptual modeling with ML
			&  \begin{enumerate}
				\item Entendimiento del problema
				\item Recopilación de datos
				\item Ingeniería de datos
				\item Entrenamiento del modelo
				\item Optimización del modelo
				\item Integración del modelo
				\item Evaluación del modelo
				\item Toma de decisiones analíticas   
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología determina si los datos de entrenamiento de un modelo de ML son representativos del dominio.
				\item Esta metodología permite que la toma de decisiones tenga un soporte solido en los datos para generar estrategias claves en la transformación digital.
				\item Esta metodología considera que la unión del modelado conceptual con el ML contribuye en la mejora de la interpretabilidad de los algoritmos de aprendizaje automático mediante el uso de modelos conceptuales.
			\end{enumerate}
			\\ \hline
			
			%--------------------------------------------------------------------------------
			\citep{Grady2017}
			&Data Science Edge (DSE)
			&  \begin{enumerate}
				\item Planificar
				\item Recopilar
				\item Seleccionar
				\item Analizar
				\item Actuar
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología se alinea con los nuevos cambios tecnológicos e implementa la agilidad en el ciclo de vida del análisis avanzado y el desarrollo de sistemas de ML.
				\item Esta metodología permite la retroalimentación constante con los interesados del proyecto de ciencia de datos para validar el estado actual e influir en su evolución hacia un estado final que cumpla con los requerimientos y objetivos de un dominio específico.
				\item Esta metodología proporciona una guía de las actividades que son fundamentales para generar de forma eficiente un producto mínimo viable para los interesados en un proyecto basado en ciencia de datos.
			\end{enumerate}
			\\ \hline
			
			%--------------------------------------------------------------------------------
			\citep{Sfaxi2020}
			&DECIDE
			&  \begin{enumerate}
				\item Identificación y motivación
				\item Definición de los objetivos
				\item Diseño y desarrollo
				\item Demostración
				\item Evaluación
				\item Comunicación
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología se fundamenta en un diseño ágil basado en eventos y datos para proyectos decisionales de Big Data. 
				\item Esta metodología ayuda a las organizaciones a determinar los objetivos comerciales y analíticos deseados según la toma de decisiones con base al análisis de datos.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Pacheco2014}
			&MIDANO
			&  \begin{enumerate}
				\item Conocimiento de la organización
				\item Preparación y tratamiento de datos
				\item Desarrollo de herramientas de MD
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología es utilizada para el desarrollo de aplicaciones de Minería de Datos (MD) basados en el análisis organizacional.
				\item Esta metodología pretende abarcar el dominio de conocimiento que puede encontrarse en una organización e integrarlo una vista mineable operativa (VMO) y una vista mineable conceptual (VMC).
				\item Esta metodología tiene como objetivo detallar las variables relevantes de diversos problemas de estudio, a partir de escenarios futuros definidos en el dominio de la organización.
			\end{enumerate}
			\\ \hline
			
			%--------------------------------------------------------------------------------
			\citep{Costa2020}
			&POST-DS
			&  \begin{enumerate}
				\item Comprensión del negocio
				\item Comprensión de los datos
				\item Preparación de los datos
				\item Modelado
				\item Evaluación 
				\item Implementación 
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología está basada en la metodología CRISP-DM, pero con la diferencia de que permite la identificación de procesos, organización, programación y herramientas para la gestión de proyectos de ciencia de datos a través de componentes específicos.
				\item Esta metodología ejecuta cada una de las fases tradicionales de un proceso en ciencia de datos teniendo como eje cada una de las fases necesarias para la gestión de proyectos.
				\item Esta metodología tiene como componentes claves el cumplimiento de actividades establecidas en un cronograma base generado a través del alcance y costos de un proyecto en ciencia de datos.
			\end{enumerate}
		\end{tabular}
	\end{threeparttable}
\end{table*}

\begin{table*}
	\begin{threeparttable}	
		\begin{tabular}{ p{2cm} p{4cm} p{5cm} p{6cm}  } \toprule
			%--------------------------------------------------------------------------------
			\citep{Watson2017}
			&SANZU
			&  \begin{enumerate}
				\item Recopilación de datos
				\item Manipulación de datos
				\item Análisis estadístico
				\item Retroalimentación
				\item Toma de decisiones 
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología sirve como punto de referencia para evaluar el rendimiento de las operaciones individuales que impactan en el análisis de datos.
				\item Esta metodología permite representar casos de uso del mundo real para modelar las aplicaciones cuyos dominios ejecutan un flujo de trabajo específico.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Saltz2019}
			&SKI
			&  \begin{enumerate}
				\item Crear
				\item Observar
				\item Analizar
			\end{enumerate}
			& \begin{enumerate}
				\item En esta metodología las etapas crear, observar y analizar garantizan que el trabajo que se requiere para la recopilación y el análisis de datos se incorpore directamente a las tareas de un equipo para una iteración de una tarea determinada.  
				\item Esta metodología en comparación con SCRUM, define una iteración centrada en la capacidad y no basada en el tiempo para brindarle a un equipo de análisis de datos la capacidad de ejecutar pequeñas iteraciones lógicas con una duración desconocida
				\item Esta metodología proporciona una guía clara que permite a los equipos de análisis de datos aprovechar al máximo los beneficios de Kanban de manera más fácil y confiable.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Safhi2019}
			& KDD
			&  \begin{enumerate}
				\item Selección de datos  
				\item Pre-procesamiento de datos 
				\item Transformación de datos
				\item Minera de datos 
				\item Evaluación/Interpretación
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología permite extracción de conocimiento oculto en una gran volumen de datos. 
				\item Esta metodología requiere un conocimiento previo relevante, una breve comprensión del dominio y la definición de los principales objetivos a cumplir con el análisis de datos. 
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------
			\citep{Shafique2014}
			& SEMMA
			&  \begin{enumerate}
				\item Muestreo 
				\item Exploración
				\item Modificación
				\item Modelado
				\item Evaluación
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología fue desarrollada por la compañía SAS para comprender, organizar, desarrollar y mantener proyectos de minería de datos.
				\item Esta metodología proporciona las soluciones a los problemas basados en objetivos definidos en el ámbito empresarial dependiendo de su dominio.
			\end{enumerate}
			\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Lei2020}
			& Agile data science in healthcare 
			&  \begin{enumerate}
				\item Definición de preguntas clínicas 
				\item Adquisición y validación de datos
				\item Desarrollo del Modelo predictivo
				\item Retroalimentación medica
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología fomenta el despliegue continuo de modelos predictivos en entornos clínicos durante el cual los científicos de datos pueden reunirse con los médicos y recibir comentarios sobre el rendimiento del modelo.
				\item Esta metodología utiliza la perspicacia del médico unida a la ciencia de datos para determinar si los resultados del modelo son creíbles
			\end{enumerate}
					\\ \hline
			%--------------------------------------------------------------------------------
			\citep{Rollins2015}
			& IBM Foundational Methodology for Data Science
			&  \begin{enumerate}
				\item Comprensión del negocio
				\item Enfoque analítico
				\item Requisitos de datos
				\item Recopilación de datos
				\item Comprensión de datos
				\item Preparación de datos
				\item Modelado
				\item Evaluación
				\item Implementación
				\item Retroalimentación
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología tiene algunas similitudes con las metodologías reconocidas para la minería de datos, pero pone el énfasis en varias de las nuevas prácticas en la ciencia de datos.
				\item Esta metodología los modelos se mejoran y se adaptan constantemente a las condiciones cambiantes a través de retroalimentación, ajustes y re-implementaciones. De esta manera, tanto el modelo como su trabajo pueden proporcionar un valor continuo a la organización mientras la solución sea necesaria.
			\end{enumerate}
			
		\end{tabular}
	\end{threeparttable}
\end{table*}
\begin{table*}
	\begin{threeparttable}	
		\begin{tabular}{ p{2cm} p{4cm} p{5cm} p{6cm}  } \toprule
			%---------------------------------------------------------------
			\citep{Dastgerdi2021}
			& DataOps Manifesto
			&  \begin{enumerate}
				 \item Ideación
				 \item Iniciación
				 \item investigación/desarrollo
				 \item Transición/producción
				 \item Retirada
			\end{enumerate}
			& \begin{enumerate}
				\item En esta metodología las etapas son adaptables al contexto y el dominio de la organización para ayudar a los equipos de análisis de datos a ser más colaborativos en los ciclos de retroalimentación para lograr resultados más eficaces. 
				\item Esta metodología se basa en la experiencia laboral de varias organizaciones y los problemas tradicionales al momento de entregar tiempos de ciclo rápidos para una alta gama de análisis de datos con un manifiesto de calidad válido.
			\end{enumerate}
			\\ \hline
			%---------------------------------------------------------------
			\citep{Chen2016}
			& AABA
			&  \begin{enumerate}
		     \item Catálogo de diseño conceptual
		     \item Arquitectura BDD 
		     \item Implementación
		     \item Pruebas
		     \item Despliegue/retroalimentación 
		     \item Descubrimiento del valor
			\end{enumerate}
			& \begin{enumerate}
				\item Esta metodología ofrece una heurística que permite que la arquitectura sea exhaustiva para todas las partes interesadas , en donde el equipo de ingeniería se centre en las tareas importantes, como la validación del valor y la anticipación del cambio.
				\item Esta metodología abarca aspectos importantes como: la planificación, la estimación, el coste , el calendario, el apoyo a la experimentación y el uso de la metodología DevOps para la entrega rápida y continua de un producto de valor.
			\end{enumerate}
			\\ \hline
			%---------------------------------------------------------------
			\citep{Li2016}
			& KDDA
			&  \begin{enumerate}
				\item Formulación del problema
				\item Comprensión empresarial
				\item Comprensión de datos
				\item Preparación de datos
				\item Modelado
				\item Evaluación
				\item Despliegue 
				\item Mantenimiento
			\end{enumerate}
			& \begin{enumerate}
			\item Esta metodología estructura su proceso en el diseño de la concha de caracol y asimila tanto las bases de conocimientos de KDD existentes como la experiencia analítica del mundo real de los investigadores. 
			\item Esta metodología enmarca las diferencias entre los proyectos tradicionales de minería de datos en un entorno de toma de decisiones impulsado por big data identificando los pasos que faltan en la metodología KDD.
			\item Esta metodología hereda la representación del ciclo de vida del proyecto de la metodología CRISP-DM con la diferencia de que no existen  secuencias estrictamente definidas entre las fases, por lo que cada fase incluye diferentes tareas, y el resultado de cada tarea determina la fase o tareas particulares a realizar en una fase determinada. 
			\end{enumerate}
			\\ \hline
	
		\end{tabular}
	\end{threeparttable}
\end{table*}

\citep{Xu2021} aseguran que la ciencia de datos constituye la tecnología central del análisis y procesamiento de Big data que contiene un enfoque eficaz para dar valor a los datos generando oportunidades sin precedentes en al menos cuatro aspectos: la innovación en la gestión, el desarrollo industrial, el descubrimiento científico y el desarrollo de disciplinas. Ademas, consideran que en la ciencia de datos, la descripción de dichos datos necesita modelado, en donde el modelado es entendido como la formalización de objetos, objetivos y métodos de procesamiento. Por otra parte, consideran que el análisis es un proceso de juzgar las propiedades teóricas de la viabilidad, precisión, complejidad y eficiencia para realizar transformación de los datos en información, la información en conocimiento y  el conocimiento en toma de decisiones, por lo tanto, para dar valor a los datos una metodología debe cumplir las siguientes fases: recopilación, convergencia, almacenamiento, administración, consulta, clasificación, extracción, análisis  y la realización de descubrimientos científicos. Por consiguiente, los autores consideran que una  metodología en ciencia de datos se resume como un híbrido de modelado, análisis, cálculo y aprendizaje, en donde el objetivo fundamental es realizar la cognición y el control del mundo real a través de  la transformación de los datos.

\citep{Pacheco2014} basados en la ciencia de datos, mejoran la fase de preparación y tratamiento de datos de una metodología para el desarrollo de aplicaciones de Minería de Datos(MD) basados en el análisis organizacional, denominada \textit{MIDANO}. Esta metodología consta de tres fases: conocimiento de la organización,  preparación y tratamiento de los datos y el desarrollo de herramientas de MD. Todas las fases y actividades de esta metodología pretenden abarcar el dominio de conocimiento que puede encontrarse en una organización, por lo que es necesario tener los datos integrados en una sola vista, la cual normalmente es conocida como \textit{Vista Minable}, que esta compuesta por una tabla con todas las variables del proceso y los datos a considerar en el estudio de MD. Sin embargo, en las metodologías actuales no muestran cómo lograr una vista minable adecuada, es por esto que los autores para mejorar el proceso definen dos tipos de vista minable: Vista Minable Conceptual (VMC), la cual describe en detalle cada una de las variables a tomar en cuenta para la tarea de MD  y una Vista Mineable Operativa (VMO) que surge como el resultado de cargar los datos del historial y de realizar la etapa de tratamiento de datos. Tanto en la VMC, como en la VMO, se identifican determinadas variables llamadas \textit{variables objetivo} que permiten la consecución de los objetivos de MD, ya que las mismas son las que se desean predecir, clasificar, calcular, inferir según el dominio del problema de estudio. En conclusión, los autores utilizaron el conjunto de formalismos teóricos, métodos y técnicas, para el tratamiento de grandes volúmenes de datos ofrecidos por las metodologías en ciencia de datos, para detallar las variables relevantes de diversos problemas de estudio, a partir de escenarios futuros definidos en el dominio de la organización.

\citep{Costa2020} proponen una metodología orientada a procesos para ayudar a la gestión de proyectos de ciencia de datos denominada \textit{POST-DS}\footnote{Process Organization and Scheduling electing Tools for Data Science}. Esta metodología describe la secuencia de actividades realizadas en un proyecto de ciencia de datos y se basa en el ciclo de vida de la metodología CRISP-DM la cual proporciona un plan completo para realizar un proyecto de minería de datos basada en las siguientes fases: comprensión del negocio, comprensión de los datos, preparación de los datos, modelado, evaluación e implementación. La metodología POST-DS está inspirada particularmente en la metodología CRISP-DM, pero con la diferencia de que permite la identificación de procesos, organización, programación y herramientas para la gestión de proyectos de ciencia de datos a través de componentes específicos para: la asignación de roles, el ajuste de expectativas, la definición del alcance del proyecto, los costos y el tiempo.Para lograrlo, esta metodología consta de las siguientes fases establecidas a través de un cronograma base: comprensión del negocio, comprensión de los datos, preparación de los datos, modelado, evaluación y despliegue. En conclusión, esta metodología ejecuta cada una de las fases tradicionales de un proceso en ciencia de datos teniendo como eje cada una de las fases necesarias para la gestión de proyectos.

\citep{Watson2017} aseguran que la variedad de sistemas actuales de análisis de datos comerciales y de código abierto difiere significativamente, en términos de características disponibles, funcionalidad y escalabilidad, de los sistemas de análisis de datos que respaldan el flujo de trabajo de la ciencia de datos. Los autores consideran, que se puede utilizar un punto de referencia (benchmark) para evaluar la funcionalidad y el rendimiento de un sistema. Sin embargo afirman, que no existe un punto de referencia estándar para evaluar la capacidad de estos sistemas para hacer ciencia de datos, aunque existan categorías de benchmark que están asociadas, tales como: el benchmark de bases de datos relacionales, el benchmark de sistemas de Big data y el benchmark de análisis específicos del dominio. Por esta razón, los autores presentan una metodología denominada \textit{Sanzu}, la cual permite evaluar sistemas que ejecuten tareas de procesamiento y análisis de datos. Esta metodología se basa en el flujo de trabajo tradicional de las ciencias de datos, el cual se compone de las siguientes fases: recopilación de datos, manipulación de datos, análisis estadístico, retroalimentación  de resultados y toma de decisiones. Adicionalmente, esta metodología se compone de dos tipos de enfoques: el enfoque \textit{micro-benchmark} que consta de seis fases: entrada y salida de archivos planos, mantenimiento de datos, estadísticas descriptivas, estadísticas de distribución e inferenciales, análisis de series de tiempo y ML. Este enfoque esta destinado a probar las tareas básicas en múltiples sistemas de análisis de datos. Dichas tareas son comunes y se utilizan todos los días para resolver problemas del mundo real y de la industria. Por otra parte, el enfoque \textit{macro-benchmark} tiene como objetivo modelar las aplicaciones cuyos dominios ejecutan el flujo de trabajo de la ciencia de datos debido al crecimiento del volumen de datos y evalúa el desempeño de los sistemas de datos de cada una. En conclusión, la metodología Sanzu sirve como punto de referencia en ciencia de datos para evaluar el rendimiento de las operaciones individuales que impactan en el análisis de datos y para representar casos de uso del mundo real.

\citep{Microsoft2022} propone una metodología llamada TDSP\footnote{Team Data Science Process}, la cuál permite ejecutar soluciones ágiles e iterativas en el análisis predictivo y el desarrollo de aplicaciones inteligentes de manera eficiente. Esta metodología incluye las mejores prácticas y estructuras de Microsoft y otros líderes de la industria para ayudar a lograr una implementación exitosa de iniciativas de ciencia de datos teniendo como base el trabajo en equipo y los criterios relevantes del manifiesto ágil. El objetivo es ayudar a que las empresas aprovechen al máximo los beneficios del análisis de datos. Dicho lo anterior, la metodología TDSP proporciona un ciclo de vida que se compone de cinco etapas principales que se ejecutan de forma iterativa: comprensión del negocio, adquisición y compresión de datos, modelado, despliegue y aceptación de cliente. Hay que mencionar, que TDSP esta sujeta al uso de la herramienta Microsoft Azure DevOps para realizar todo el proceso de ciencia de datos desde el entendimiento del negocio hasta la entrega de un producto de valor a un cliente especifico. En conclusión, el ciclo de vida de TDSP fue diseñado para proyectos destinados a generar aplicaciones inteligentes en donde el análisis predictivo se realiza a través de modelos de ML e AI. Sin embargo, puede ser utilizado para llevar a cabo proyectos en donde el propósito es tomar decisiones según el análisis obtenido en el reconocimiento de patrones de un conjunto de datos específicos.

\citep{Saltz2019} proponen un marco ágil para la ciencia de datos denominado SKI\footnote{Structured Kanban Iteration}. Esta metodología adopta la filosofía de tableros Kanban  para proporcionar un proceso de iteración estructurado para que los equipos de análisis de datos exploren y aprendan de manera incremental a través de pruebas de hipótesis.En general, en esta metodología la agilidad esta basada en una secuencia de ciclos iterativos de experimentación y adaptación a través de la implementación y el análisis de los resultados. Los equipos de SKI usan un tablero visual y se enfocan en trabajar en un elemento específico durante una iteración, que se basa en tareas, no en bloques de tiempo. Por lo tanto, una iteración se alinea más estrechamente con el concepto de extraer tareas de manera prioritaria, cuando el equipo tiene capacidad. Cada iteración puede verse como la validación o el rechazo de una hipótesis específica. A nivel general, una iteración se compone de tres etapas principales: Crear, Observar y Analizar. Las etapas anteriores se enfocan en garantizar que el trabajo que se requiere para la recopilación y el análisis de datos se incorpora directamente a las tareas del equipo para una iteración determinada. En conclusión, en comparación con Scrum, SKI define una iteración centrada en la capacidad y no en el tiempo, para brindarle a un equipo de análisis de datos la ejecución de pequeñas iteraciones lógicas con una duración desconocida.

\citep{Lei2020} proponen una metodología que realiza la unión de Scrum y Kanban (Scrumban) para la investigación ágil en el cuidado de la salud. La metodología propuesta consta de cuatro fases: Definición de preguntas clínicas, adquisición y validación de datos, desarrollo del modelo predictivo y retroalimentación medica. Los autores consideran que para que se cumpla el enfoque ágil en esta metodología debe existir una colaboración continua entre científicos de datos y médicos. Ademas, debe existir almacenamiento y computación basados en la nube para proporcionar una plataforma común para acceder a los datos y modelos. Los problemas complejos se pueden dividir en tareas que pueden visualizar tanto los científicos de datos como los médicos, lo que les permite comprender mejor el trabajo que los científicos de datos deben realizar dentro de cada ciclo del Sprint. La metodología fomenta el despliegue continuo de modelos predictivos en entornos clínicos durante el cual los científicos de datos pueden reunirse con los médicos y recibir comentarios sobre el rendimiento del modelo. Ademas, la perspicacia del médico se puede aprovechar para determinar si los resultados del modelo son creíbles. En conclusion, esta metodología prescribe un proceso rápido y de mejora continua que permite a los médicos comprender el trabajo de los científicos de datos y evaluar regularmente el rendimiento de un modelo predictivo en entornos clínicos.

\citep{Rollins2015} propone la metodología IBM Foundational Methodology for Data Science, la cual tiene algunas similitudes con las metodologías mas utilizadas en minería de datos, con la diferencia que se enfatiza en las prácticas mas recientes en la ciencia de datos, como el uso de grandes volúmenes de datos, la incorporación de la analítica de texto en el modelado predictivo y la automatización de procesos. La metodología consta de diez etapas: comprensión del negocio, enfoque analítico, requisitos de datos, recopilación de datos, comprensión de datos, preparación de datos, modelado, evaluación, implementación y retroalimentación. Estas etapas  forman un proceso iterativo para el uso de datos con el propósito  descubrir conocimiento oculto. En conclusion, esta metodología ilustra la naturaleza iterativa del proceso de resolución de problemas en donde, los científicos de datos vuelven frecuentemente a etapas previas para realizar ajustes a medida que van aprendiendo de los datos y el modelado.  

\citep{Dastgerdi2021} realizaron una revisión de diversas metodologías utilizadas en análisis de datos, en donde resaltan el ciclo de vida de la ciencia de datos basado en el DataOps Manifesto. Esta metodología ayuda a caracterizar qué prácticas ágiles, eventos, artefactos y roles son valiosos para agregar valor a los datos de la organización diariamente. Esta metodología se compone de cinco etapas generales: ideación, iniciación, investigación/desarrollo, transición/producción y Retirada. Estas etapas  ayudan a los equipos de análisis de datos a ser más adaptables y colaborativos en los ciclos de retroalimentación para lograr resultados de manera eficiente. En conclusión, esta metodología se basa en la experiencia laboral de varias organizaciones y los problemas tradicionales al momento de entregar tiempos de ciclo rápidos para una alta gama de análisis de datos con un manifiesto de calidad válido.

\citep{Chen2016} proponen una metodología denominada    AABA\footnote{Architecture-centric Agile Big data Analytics} para el análisis ágil de Big data centrado en la arquitectura. Esta metodología consta de seis etapas: catálogo de diseño conceptual, arquitectura BDD\footnote{Big Data system Design}, implementación, pruebas, despliegue/retroalimentación y descubrimiento del valor. Esta metodología se basa en el desempeño de la arquitectura de software como factor clave de agilidad. AABA proporciona una base para el descubrimiento de valor en los datos con las partes interesadas y abarca aspectos importantes como: la planificación, la estimación, el coste , el calendario, el apoyo a la experimentación y el uso de la metodología DevOps para la entrega rápida y continua de un producto de valor. En conclusión, esta metodología se centra con la colaboración estrecha entre los científicos de datos, las partes interesadas, el arquitecto de software y otros ingenieros clave como los diseñadores de bases de datos, esto con el propósito de determinar la propuesta de valor para el sistema que se está construyendo en función de de la mejora continua y el cumplimiento de los objetivos comerciales.

\citep{Li2016} proponen una mejora para la metodología KDD denominada KDDA\footnote{Knowledge Discovery via Data Analytics}. Esta metodología se basa en el descubrimiento de conocimiento a través del análisis de datos y no de la minería de datos. Para lograrlo consta de un proceso de concha de caracol, el cual se compone de ochos fases: formulación del problema, comprensión empresarial, comprensión de datos, preparación de datos, modelado, evaluación ,despliegue y mantenimiento. Esta metodología hereda la representación del ciclo de vida de la metodología CRISP-DM, con la diferencia de que no tiene secuencias estrictamente definidas entre las fases. Cada fase incluye diferentes tareas, y el resultado de cada tarea determina la fase o tareas particulares de una fase a realizar, lo que permite que un resultado específico de la fase de modelado puede requerir volver a la comprensión del negocio, la comprensión de los datos, la preparación de los datos o ir directamente a la evaluación. En conclusión, esta metodología utiliza las prácticas de descubrimiento de conocimiento en el entorno analítico de una organización abarcando no solo conocimientos técnicos de TI, técnicas analíticas y algoritmos matemáticos, sino también una comprensión profunda del proceso empresarial.




